No.,Paper name,Link,Authors,Year,Source,Cited,Description
0,Omni-Scale Feature Learning for Person Re-Identification,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:qjMakFHDy7sC,"Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang",2019,"IEEE International Conference on Computer Vision (ICCV), 2019",368,"As an instance-level recognition problem, person re-identification (ReID) relies on discriminative features, which not only capture different spatial scales but also encapsulate an arbitrary combination of multiple scales. We callse features of both homogeneous and heterogeneous scales omni-scale features. In this paper, a novel deep ReID CNN is designed, termed Omni-Scale Network (OSNet), for omni-scale feature learning. This is achieved by designing a residual block composed of multiple convolutional feature streams, each detecting features at a certain scale. Importantly, a novel unified aggregation gate is introduced to dynamically fuse multi-scale features with input-dependent channel-wise weights. To efficiently learn spatial-channel correlations and avoid overfitting, the building block uses both pointwise and depthwise convolutions. By stacking such blocks layer-by-layer, our OSNet is extremely lightweight and can be trained from scratch on existing ReID benchmarks. Despite its small model size, our OSNet achieves state-of-the-art performance on six person-ReID datasets. Code and models are available at: https://github. com/KaiyangZhou/deep-person-reid."
1,Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:9yKSN-GCB0IC,"Kaiyang Zhou, Yu Qiao, Tao Xiang",2018,"AAAI Conference on Artificial Intelligence (AAAI), 2018",219,"Video summarization aims to facilitate large-scale video browsing by producing short, concise summaries that are diverse and representative of original videos. In this paper, we formulate video summarization as a sequential decision-making process and develop a deep summarization network (DSN) to summarize videos. DSN predicts for each video frame a probability, which indicates how likely a frame is selected, and then takes actions based on the probability distributions to select frames, forming video summaries. To train our DSN, we propose an end-to-end, reinforcement learning-based framework, where we design a novel reward function that jointly accounts for diversity and representativeness of generated summaries and does not rely on labels or user interactions at all. During training, the reward function judges how diverse and representative the generated summaries are, while DSN strives for earning higher rewards by learning to produce more diverse and more representative summaries. Since labels are not required, our method can be fully unsupervised. Extensive experiments on two benchmark datasets show that our unsupervised method not only outperforms other state-of-the-art unsupervised methods, but also is comparable to or even superior than most of published supervised approaches."
2,Learning to Generate Novel Domains for Domain Generalization,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:YsMSGLbcyi4C,"Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang",2020,"European Conference on Computer Vision (ECCV), 2020",82," This paper focuses on domain generalization (DG), the task of learning from multiple source domains a model that generalizes well to unseen domains. A main challenge for DG is that the available source domains often exhibit limited diversity, hampering the model’s ability to learn to generalize. We therefore employ a data generator to synthesize data from pseudo-novel domains to augment the source domains. This explicitly increases the diversity of available training domains and leads to a more generalizable model. To train the generator, we model the distribution divergence between source and synthesized pseudo-novel domains using optimal transport, and maximize the divergence. To ensure that semantics are preserved in the synthesized data, we further impose cycle-consistency and classification losses on the generator. Our method, L2A-OT (Learning to Augment by Optimal Transport …"
3,Domain Generalization with MixStyle,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:WF5omc3nYNoC,"Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang",2021,"International Conference on Learning Representations (ICLR), 2021",81,"Though convolutional neural networks (CNNs) have demonstrated remarkable ability in learning discriminative features, they often generalize poorly to unseen domains. Domain generalization aims to address this problem by learning from a set of source domains a model that is generalizable to any unseen domain. In this paper, a novel approach is proposed based on probabilistically mixing instance-level feature statistics of training samples across source domains. Our method, termed MixStyle, is motivated by the observation that visual domain is closely related to image style (e.g., photo vs.~sketch images). Such style information is captured by the bottom layers of a CNN where our proposed style-mixing takes place. Mixing styles of training instances results in novel domains being synthesized implicitly, which increase the domain diversity of the source domains, and hence the generalizability of the trained model. MixStyle fits into mini-batch training perfectly and is extremely easy to implement. The effectiveness of MixStyle is demonstrated on a wide range of tasks including category classification, instance retrieval and reinforcement learning."
4,Domain Generalization in Vision: A Survey,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:ufrVoPGSRksC,"Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy",2021,"arXiv preprint arXiv:2103.02503, 2021",75,"Generalization to out-of-distribution (OOD) data is a capability natural to humans yet challenging for machines to reproduce. This is because most learning algorithms strongly rely on the i.i.d.~assumption on source/target data, which is often violated in practice due to domain shift. Domain generalization (DG) aims to achieve OOD generalization by using only source data for model learning. Since first introduced in 2011, research in DG has made great progresses. In particular, intensive research in this topic has led to a broad spectrum of methodologies, e.g., those based on domain alignment, meta-learning, data augmentation, or ensemble learning, just to name a few; and has covered various vision applications such as object recognition, segmentation, action recognition, and person re-identification. In this paper, for the first time a comprehensive literature review is provided to summarize the developments in DG for computer vision over the past decade. Specifically, we first cover the background by formally defining DG and relating it to other research fields like domain adaptation and transfer learning. Second, we conduct a thorough review into existing methods and present a categorization based on their methodologies and motivations. Finally, we conclude this survey with insights and discussions on future research directions."
5,Deep Domain-Adversarial Image Generation for Domain Generalisation,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:Tyk-4Ss8FVUC,"Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang",2020,"AAAI Conference on Artificial Intelligence (AAAI), 2020",75,"Machine learning models typically suffer from the domain shift problem when trained on a source dataset and evaluated on a target dataset of different distribution. To overcome this problem, domain generalisation (DG) methods aim to leverage data from multiple source domains so that a trained model can generalise to unseen domains. In this paper, we propose a novel DG approach based on Deep Domain-Adversarial Image Generation (DDAIG). Specifically, DDAIG consists of three components, namely a label classifier, a domain classifier and a domain transformation network (DoTNet). The goal for DoTNet is to map the source training data to unseen domains. This is achieved by having a learning objective formulated to ensure that the generated data can be correctly classified by the label classifier while fooling the domain classifier. By augmenting the source training data with the generated unseen domain data, we can make the label classifier more robust to unknown domain changes. Extensive experiments on four DG datasets demonstrate the effectiveness of our approach."
6,Torchreid: A Library for Deep Learning Person Re-Identification in PyTorch,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:IjCSPb-OGe4C,"Kaiyang Zhou, Tao Xiang",2019,"arXiv preprint arXiv:1910.10093, 2019",60,"Person re-identification (re-ID), which aims to re-identify people across different camera views, has been significantly advanced by deep learning in recent years, particularly with convolutional neural networks (CNNs). In this paper, we present Torchreid, a software library built on PyTorch that allows fast development and end-to-end training and evaluation of deep re-ID models. As a general-purpose framework for person re-ID research, Torchreid provides (1) unified data loaders that support 15 commonly used re-ID benchmark datasets covering both image and video domains, (2) streamlined pipelines for quick development and benchmarking of deep re-ID models, and (3) implementations of the latest re-ID CNN architectures along with their pre-trained models to facilitate reproducibility as well as future research. With a high-level modularity in its design, Torchreid offers a great flexibility to allow easy extension to new datasets, CNN models and loss functions."
7,Learning Generalisable Omni-Scale Representations for Person Re-Identification,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:UeHWp8X0CEIC,"Kaiyang Zhou, Yongxin Yang, Andrea Cavallaro, Tao Xiang",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2021",56,"An effective person re-identification (re-ID) model should learn feature representations that are both discriminative, for distinguishing similar-looking people, and generalisable, for deployment across datasets without any adaptation. In this paper, we develop novel CNN architectures to address both challenges. First, we present a re-ID CNN termed omni-scale network (OSNet) to learn features that not only capture different spatial scales but also encapsulate a synergistic combination of multiple scales, namely omni-scale features. The basic building block consists of multiple convolutional streams, each detecting features at a certain scale. For omni-scale feature learning, a unified aggregation gate is introduced to dynamically fuse multi-scale features with channel-wise weights. OSNet is lightweight as its building blocks comprise factorised convolutions. Second, to improve generalisable feature learning, we …"
8,Domain Adaptive Ensemble Learning,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:Y0pCki6q_DkC,"Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang",2021,"IEEE Transactions on Image Processing (TIP), 2021",40,"The problem of generalizing deep neural networks from multiple source domains to a target one is studied under two settings: When unlabeled target data is available, it is a multi-source unsupervised domain adaptation (UDA) problem, otherwise a domain generalization (DG) problem. We propose a unified framework termed domain adaptive ensemble learning (DAEL) to address both problems. A DAEL model is composed of a CNN feature extractor shared across domains and multiple classifier heads each trained to specialize in a particular source domain. Each such classifier is an expert to its own domain but a non-expert to others. DAEL aims to learn these experts collaboratively so that when forming an ensemble, they can leverage complementary information from each other to be more effective for an unseen target domain. To this end, each source domain is used in turn as a pseudo-target-domain with its …"
9,Learning to Prompt for Vision-Language Models,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:5nxA0vEk-isC,"Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",2021,"arXiv preprint arXiv:2109.01134, 2021",33,"Vision-language pre-training has recently emerged as a promising alternative for representation learning. It shifts from the tradition of using images and discrete labels for learning a fixed set of weights, seen as visual concepts, to aligning images and raw text for two separate encoders. Such a paradigm benefits from a broader source of supervision and allows zero-shot transfer to downstream tasks since visual concepts can be diametrically generated from natural language, known as prompt. In this paper, we identify that a major challenge of deploying such models in practice is prompt engineering. This is because designing a proper prompt, especially for context words surrounding a class name, requires domain expertise and typically takes a significant amount of time for words tuning since a slight change in wording could have a huge impact on performance. Moreover, different downstream tasks require specific designs, further hampering the efficiency of deployment. To overcome this challenge, we propose a novel approach named context optimization (CoOp). The main idea is to model context in prompts using continuous representations and perform end-to-end learning from data while keeping the pre-trained parameters fixed. In this way, the design of task-relevant prompts can be fully automated. Experiments on 11 datasets show that CoOp effectively turns pre-trained vision-language models into data-efficient visual learners, requiring as few as one or two shots to beat hand-crafted prompts with a decent margin and able to gain significant improvements when using more shots (e.g., at 16 shots the average gain is around 17% with the …"
10,Video Summarisation by Classification with Deep Reinforcement Learning,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:2osOgNQ5qMEC,"Kaiyang Zhou, Tao Xiang, Andrea Cavallaro",2018,"British Machine Vision Conference  (BMVC), 2018",25,"Most existing video summarisation methods are based on either supervised or unsupervised learning. In this paper, we propose a reinforcement learning-based weakly supervised method that exploits easy-to-obtain, video-level category labels and encourages summaries to contain category-related information and maintain category recognisability. Specifically, We formulate video summarisation as a sequential decision-making process and train a summarisation network with deep Q-learning (DQSN). A companion classification network is also trained to provide rewards for training the DQSN. With the classification network, we develop a global recognisability reward based on the classification result. Critically, a novel dense ranking-based reward is also proposed in order to cope with the temporally delayed and sparse reward problems for long sequence reinforcement learning. Extensive experiments on two benchmark datasets show that the proposed approach achieves state-of-the-art performance."
11,Detecting humans in RGB-D data with CNNs,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:u-x6o8ySG0sC,"Kaiyang Zhou, Adeline Paiement, Majid Mirmehdi",2017,"IAPR International Conference on Machine Vision Applications (MVA), 2017",23,"We address the problem of people detection in RGB-D data where we leverage depth information to develop a region-of-interest (ROI) selection method that provides proposals to two color and depth CNNs. To combine the detections produced by the two CNNs, we propose a novel fusion approach based on the characteristics of depth images. We also present a new depth-encoding scheme, which not only encodes depth images into three channels but also enhances the information for classification. We conduct experiments on a publicly available RGB-D people dataset and show that our approach outperforms the baseline models that only use RGB data."
12,Generalized out-of-distribution detection: A survey,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:8k81kl-MbHgC,"Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu",2021,"arXiv preprint arXiv:2110.11334, 2021",17,"Out-of-distribution (OOD) detection is critical to ensuring the reliability and safety of machine learning systems. For instance, in autonomous driving, we would like the driving system to issue an alert and hand over the control to humans when it detects unusual scenes or objects that it has never seen before and cannot make a safe decision. This problem first emerged in 2017 and since then has received increasing attention from the research community, leading to a plethora of methods developed, ranging from classification-based to density-based to distance-based ones. Meanwhile, several other problems are closely related to OOD detection in terms of motivation and methodology. These include anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). Despite having different definitions and problem settings, these problems often confuse readers and practitioners, and as a result, some existing studies misuse terms. In this survey, we first present a generic framework called generalized OOD detection, which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD detection, and OD. Under our framework, these five problems can be seen as special cases or sub-tasks, and are easier to distinguish. Then, we conduct a thorough review of each of the five areas by summarizing their recent technical developments. We conclude this survey with open challenges and potential research directions."
13,Energy-Based Open-World Uncertainty Modeling for Confidence Calibration,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:0EnyYjriUFMC,"Yezhen Wang, Bo Li, Tong Che, Kaiyang Zhou, Dongsheng Li, Ziwei Liu",2021,"IEEE International Conference on Computer Vision (ICCV), 2021",4,"Confidence calibration is of great importance to ensure the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify the input into one of K pre-defined categories with high probability. To address this problem, we for the first time propose a novel K+ 1-way softmax formulation, which incorporates the modeling of open-world uncertainty as to the extra dimension. To unify the learning of the original K-way classification task and the extra dimension that models uncertainty, we (1) propose a novel energy-based objective function, and moreover,(2) theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration."
14,Semi-supervised domain generalization with stochastic stylematch,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:Se3iqnhoufwC,"Kaiyang Zhou, Chen Change Loy, Ziwei Liu",2021,"arXiv preprint arXiv:2106.00592, 2021",3,"Most existing research on domain generalization assumes source data gathered from multiple domains are fully annotated. However, in real-world applications, we might have only a few labels available from each source domain due to high annotation cost, along with abundant unlabeled data that are much easier to obtain. In this work, we investigate semi-supervised domain generalization (SSDG), a more realistic and practical setting. Our proposed approach, StyleMatch, is inspired by FixMatch, a state-of-the-art semi-supervised learning method based on pseudo-labeling, with several new ingredients tailored to solve SSDG. Specifically, 1) to mitigate overfitting in the scarce labeled source data while improving robustness against noisy pseudo labels, we introduce stochastic modeling to the classifier's weights, seen as class prototypes, with Gaussian distributions. 2) To enhance generalization under domain shift, we upgrade FixMatch's two-view consistency learning paradigm based on weak and strong augmentations to a multi-view version with style augmentation as the third complementary view. To provide a comprehensive study and evaluation, we establish two SSDG benchmarks, which cover a wide range of strong baseline methods developed in relevant areas including domain generalization and semi-supervised learning. Extensive experiments demonstrate that StyleMatch achieves the best out-of-distribution generalization performance in the low-data regime. We hope our approach and benchmarks can pave the way for future research on data-efficient and generalizable learning systems."
15,Mixstyle neural networks for domain generalization and adaptation,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:hqOjcs7Dif8C,"Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang",2021,"arXiv preprint arXiv:2107.02053, 2021",2,"Convolutional neural networks (CNNs) often have poor generalization performance under domain shift. One way to improve domain generalization is to collect diverse source data from multiple relevant domains so that a CNN model is allowed to learn more domain-invariant, and hence generalizable representations. In this work, we address domain generalization with MixStyle, a plug-and-play, parameter-free module that is simply inserted to shallow CNN layers and requires no modification to training objectives. Specifically, MixStyle probabilistically mixes feature statistics between instances. This idea is inspired by the observation that visual domains can often be characterized by image styles which are in turn encapsulated within instance-level feature statistics in shallow CNN layers. Therefore, inserting MixStyle modules in effect synthesizes novel domains albeit in an implicit way. MixStyle is not only simple and flexible, but also versatile -- it can be used for problems whereby unlabeled images are available, such as semi-supervised domain generalization and unsupervised domain adaptation, with a simple extension to mix feature statistics between labeled and pseudo-labeled instances. We demonstrate through extensive experiments that MixStyle can significantly boost the out-of-distribution generalization performance across a wide range of tasks including object recognition, instance retrieval, and reinforcement learning."
16,Conditional Prompt Learning for Vision-Language Models,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:3fE2CSJIrl8C,"Kaiyang Zhou, Jingkang Yang, Chen Change Loy, Ziwei Liu",2022,"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022",1,"With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning -- a recent trend in NLP -- to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp."
17,Open-Vocabulary DETR with Conditional Matching,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:KlAtU1dfN6UC,"Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, Chen Change Loy",2022,"arXiv preprint arXiv:2203.11876, 2022",,"Open-vocabulary object detection, which is concerned with the problem of detecting novel objects guided by natural language, has gained increasing attention from the community. Ideally, we would like to extend an open-vocabulary detector such that it can produce bounding box predictions based on user inputs in form of either natural language or exemplar image. This offers great flexibility and user experience for human-computer interaction. To this end, we propose a novel open-vocabulary detector based on DETR -- hence the name OV-DETR -- which, once trained, can detect any object given its class name or an exemplar image. The biggest challenge of turning DETR into an open-vocabulary detector is that it is impossible to calculate the classification cost matrix of novel classes without access to their labeled images. To overcome this challenge, we formulate the learning objective as a binary matching one between input queries (class name or exemplar image) and the corresponding objects, which learns useful correspondence to generalize to unseen queries during testing. For training, we choose to condition the Transformer decoder on the input embeddings obtained from a pre-trained vision-language model like CLIP, in order to enable matching for both text and image queries. With extensive experiments on LVIS and COCO datasets, we demonstrate that our OV-DETR -- the first end-to-end Transformer-based open-vocabulary detector -- achieves non-trivial improvements over current state of the arts."
18,Dynamic Instance Domain Adaptation,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:kNdYIx-mwKoC,"Zhongying Deng, Kaiyang Zhou, Da Li, Junjun He, Yi-Zhe Song, Tao Xiang",2022,"arXiv preprint arXiv:2203.05028, 2022",,"Most existing studies on unsupervised domain adaptation (UDA) assume that each domain's training samples come with domain labels (e.g., painting, photo). Samples from each domain are assumed to follow the same distribution and the domain labels are exploited to learn domain-invariant features via feature alignment. However, such an assumption often does not hold true -- there often exist numerous finer-grained domains (e.g., dozens of modern painting styles have been developed, each differing dramatically from those of the classic styles). Therefore, forcing feature distribution alignment across each artificially-defined and coarse-grained domain can be ineffective. In this paper, we address both single-source and multi-source UDA from a completely different perspective, which is to view each instance as a fine domain. Feature alignment across domains is thus redundant. Instead, we propose to perform dynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network with adaptive convolutional kernels is developed to generate instance-adaptive residuals to adapt domain-agnostic deep features to each individual instance. This enables a shared classifier to be applied to both source and target domain data without relying on any domain annotation. Further, instead of imposing intricate feature alignment losses, we adopt a simple semi-supervised learning paradigm using only a cross-entropy loss for both labeled source and pseudo labeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art performance on several commonly used single-source and multi-source UDA datasets including Digits, Office …"
19,Domain Attention Consistency for Multi-Source Domain Adaptation,https://scholar.google.com//citations?view_op=view_citation&hl=vi&user=gRIejugAAAAJ&citation_for_view=gRIejugAAAAJ:MXK_kJrjxJIC,"Zhongying Deng, Kaiyang Zhou, Yongxin Yang, Tao Xiang",2021,"British Machine Vision Conference (BMVC), 2021",,"Most existing multi-source domain adaptation (MSDA) methods minimize the distance between multiple source-target domain pairs via feature distribution alignment, an approach borrowed from the single source setting. However, with diverse source domains, aligning pairwise feature distributions is challenging and could even be counter-productive for MSDA. In this paper, we introduce a novel approach: transferable attribute learning. The motivation is simple: although different domains can have drastically different visual appearances, they contain the same set of classes characterized by the same set of attributes; an MSDA model thus should focus on learning the most transferable attributes for the target domain. Adopting this approach, we propose a domain attention consistency network, dubbed DAC-Net. The key design is a feature channel attention module, which aims to identify transferable features (attributes). Importantly, the attention module is supervised by a consistency loss, which is imposed on the distributions of channel attention weights between source and target domains. Moreover, to facilitate discriminative feature learning on the target data, we combine pseudo-labeling with a class compactness loss to minimize the distance between the target features and the classifier's weight vectors. Extensive experiments on three MSDA benchmarks show that our DAC-Net achieves new state of the art performance on all of them."
